{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial: Log Clustering using LogAI\n",
    "\n",
    "This is an example to show how to use LogAI to conduct log clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data\n",
    "\n",
    "You can use `OpensetDataLoader` to load a sample open log dataset. Here we use HDFS dataset from\n",
    "[LogHub](https://zenodo.org/record/3227177#.Y1M3LezML0o) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/root/logai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('^(?P<Date>.*?)\\\\s+(?P<Time>.*?)\\\\s+\\\\[(?P<Pid>.*?)\\\\]\\\\s+(?P<Level>.*?)\\\\s+(?P<Module>.*?):(?P<Line>.*?)\\\\s+-\\\\s+(?P<Content>.*?)$')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logline</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>trace_id</th>\n",
       "      <th>span_id</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Starting AispApplication v1.0.0 on AISPapp-194...</td>\n",
       "      <td>2023-05-12 17:30:09.025</td>\n",
       "      <td>main</td>\n",
       "      <td>org.aisp.AispApplication</td>\n",
       "      <td>INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The following profiles are active: prod</td>\n",
       "      <td>2023-05-12 17:30:09.119</td>\n",
       "      <td>main</td>\n",
       "      <td>org.aisp.AispApplication</td>\n",
       "      <td>INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiple Spring Data modules found, entering s...</td>\n",
       "      <td>2023-05-12 17:30:11.847</td>\n",
       "      <td>main</td>\n",
       "      <td>o.s.d.r.config.RepositoryConfigurationDelegate</td>\n",
       "      <td>INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bootstrapping Spring Data repositories in DEFA...</td>\n",
       "      <td>2023-05-12 17:30:11.847</td>\n",
       "      <td>main</td>\n",
       "      <td>o.s.d.r.config.RepositoryConfigurationDelegate</td>\n",
       "      <td>INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finished Spring Data repository scanning in 78...</td>\n",
       "      <td>2023-05-12 17:30:12.175</td>\n",
       "      <td>main</td>\n",
       "      <td>o.s.d.r.config.RepositoryConfigurationDelegate</td>\n",
       "      <td>INFO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             logline               timestamp  \\\n",
       "0  Starting AispApplication v1.0.0 on AISPapp-194... 2023-05-12 17:30:09.025   \n",
       "1            The following profiles are active: prod 2023-05-12 17:30:09.119   \n",
       "2  Multiple Spring Data modules found, entering s... 2023-05-12 17:30:11.847   \n",
       "3  Bootstrapping Spring Data repositories in DEFA... 2023-05-12 17:30:11.847   \n",
       "4  Finished Spring Data repository scanning in 78... 2023-05-12 17:30:12.175   \n",
       "\n",
       "  trace_id                                         span_id Severity  \n",
       "0     main                        org.aisp.AispApplication     INFO  \n",
       "1     main                        org.aisp.AispApplication     INFO  \n",
       "2     main  o.s.d.r.config.RepositoryConfigurationDelegate     INFO  \n",
       "3     main  o.s.d.r.config.RepositoryConfigurationDelegate     INFO  \n",
       "4     main  o.s.d.r.config.RepositoryConfigurationDelegate     INFO  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from logai.dataloader.openset_data_loader import OpenSetDataLoader, OpenSetDataLoaderConfig\n",
    "#File Configuration\n",
    "filepath = \"../datasets/HDFS_2000.log\"\n",
    "filepath = os.path.join(\"..\", \"datasets\", \"jeecgboot_10.log\")\n",
    "\n",
    "dataset_name = \"jeecgboot\"\n",
    "data_loader = OpenSetDataLoader(\n",
    "    OpenSetDataLoaderConfig(\n",
    "        dataset_name=dataset_name,\n",
    "        filepath=filepath)\n",
    ")\n",
    "\n",
    "logrecord = data_loader.load_data()\n",
    "\n",
    "logrecord.to_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "In preprocessing step user can retrieve and replace any regex strings and clean the raw loglines. This\n",
    "can be very useful to improve information extraction of the unstructured part of logs,\n",
    " as well as generate more structured attributes with domain knowledge.\n",
    "\n",
    "Here in the example, we use the below regex to retrieve Block IDs, IP addresses and filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from logai.preprocess.preprocessor import PreprocessorConfig, Preprocessor\n",
    "from logai.utils import constants\n",
    "\n",
    "loglines = logrecord.body[constants.LOGLINE_NAME]\n",
    "attributes = logrecord.attributes\n",
    "\n",
    "preprocessor_config = PreprocessorConfig(\n",
    "    custom_replace_list=[\n",
    "        [r\"(?<=blk_)[-\\d]+\", \"<block_id>\"],\n",
    "        [r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", \"<IP>\"],\n",
    "        [r\"(/[-\\w]+)+\", \"<file_path>\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(preprocessor_config)\n",
    "\n",
    "clean_logs, custom_patterns = preprocessor.clean_log(\n",
    "    loglines\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "After preprocessing, we call auto-parsing algorithms to automatically parse the cleaned logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "\n",
    "# parsing\n",
    "parsing_algo_params = DrainParams(\n",
    "    sim_th=0.5, depth=5\n",
    ")\n",
    "\n",
    "log_parser_config = LogParserConfig(\n",
    "    parsing_algorithm=\"drain\",\n",
    "    parsing_algo_params=parsing_algo_params\n",
    ")\n",
    "\n",
    "parser = LogParser(log_parser_config)\n",
    "parsed_result = parser.parse(clean_logs)\n",
    "\n",
    "parsed_loglines = parsed_result['parsed_logline']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Information Extraction\n",
    "\n",
    "### Vectorization for unstructured loglines\n",
    "\n",
    "Here we use `word2vec` to vectorize unstructured part of the logs. The output will be a list of\n",
    "numeric vectors that representing the semantic features of these log templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.log_vectorizer import VectorizerConfig, LogVectorizer\n",
    "\n",
    "vectorizer_config = VectorizerConfig(\n",
    "    algo_name = \"word2vec\"\n",
    ")\n",
    "\n",
    "vectorizer = LogVectorizer(\n",
    "    vectorizer_config\n",
    ")\n",
    "\n",
    "# Train vectorizer\n",
    "vectorizer.fit(parsed_loglines)\n",
    "\n",
    "# Transform the loglines into features\n",
    "log_vectors = vectorizer.transform(parsed_loglines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Categorical Encoding for log attributes\n",
    "\n",
    "We also do categorical encoding for log attributes to convert the strings into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.categorical_encoder import CategoricalEncoderConfig, CategoricalEncoder\n",
    "\n",
    "encoder_config = CategoricalEncoderConfig(name=\"label_encoder\")\n",
    "\n",
    "encoder = CategoricalEncoder(encoder_config)\n",
    "\n",
    "attributes_encoded = encoder.fit_transform(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Then we extract and concate the semantic features for both the unstructured and structured part of logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    max_feature_len=100\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "_, feature_vector = feature_extractor.convert_to_feature_vector(log_vectors, attributes_encoded, timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Here we use K-Means clustering algorithm as an example. WE set the number of clusters to 7 in\n",
    "K-Means algorithm parameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from logai.algorithms.clustering_algo.kmeans import KMeansParams\n",
    "from logai.analysis.clustering import ClusteringConfig, Clustering\n",
    "\n",
    "clustering_config = ClusteringConfig(\n",
    "    algo_name='kmeans',\n",
    "    algo_params=KMeansParams(\n",
    "        n_clusters=7\n",
    "    )\n",
    ")\n",
    "\n",
    "log_clustering = Clustering(clustering_config)\n",
    "\n",
    "log_clustering.fit(feature_vector)\n",
    "\n",
    "cluster_id = log_clustering.predict(feature_vector).astype(str).rename('cluster_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check clustering results.\n",
    "logrecord.to_dataframe().join(cluster_id).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
